{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOg/rU+OqW2RszLMyciEP71",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcjimenezb123/PFM_PronosticoDemanda/blob/master/LGBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LGBM\n",
        "===\n",
        "El modelo LGBM (Light Gradient Boosting Machine) es un algoritmo de aprendizaje automático basado en el método de gradient boosting. Fue desarrollado para ser altamente eficiente, rápido y escalable en problemas de regresión, clasificación y ranking.\n",
        "\n",
        "LightGBM es una implementación optimizada de gradient boosting que utiliza técnicas específicas para mejorar el rendimiento y reducir el tiempo de entrenamiento, especialmente en conjuntos de datos grandes con muchas características.\n",
        "\n",
        "Principales Características de LightGBM:\n",
        "---\n",
        "División por hojas (Leaf-wise Splitting):\n",
        "A diferencia de otras implementaciones de gradient boosting como XGBoost, que usan una estrategia de división por nivel (level-wise), LightGBM utiliza una estrategia de división basada en hojas. Esto significa que selecciona la hoja con la mayor ganancia de información para dividir en cada paso.\n",
        "\n",
        "* Ventaja: Mejora la precisión del modelo.\n",
        "* Desventaja: Puede causar sobreajuste en conjuntos de datos pequeños.\n",
        "\n",
        "Optimización para datos grandes:\n",
        "---\n",
        "LightGBM es capaz de manejar conjuntos de datos con millones de instancias y cientos de características de manera eficiente.\n",
        "Utiliza técnicas como histogramas para acelerar el cálculo de divisiones en los árboles.\n",
        "\n",
        "Soporte para características categóricas:\n",
        "---\n",
        "LightGBM puede manejar características categóricas directamente, evitando la necesidad de codificación (como One-Hot Encoding).\n",
        "\n",
        "Reducción de memoria:\n",
        "---\n",
        "Requiere menos memoria en comparación con otros métodos de gradient boosting.\n",
        "Soporte para paralelización:\n",
        "\n",
        "Permite paralelizar el proceso de construcción de árboles, lo que lo hace más rápido en sistemas con múltiples núcleos.\n",
        "* Ventajas:\n",
        " * Velocidad: Entrena más rápido que otros algoritmos como XGBoost.\n",
        " * Eficiencia: Maneja grandes cantidades de datos y características con menor uso de memoria.\n",
        " * Precisión: Produce resultados competitivos o superiores en muchas competiciones de aprendizaje automático.\n",
        " * Flexibilidad: Admite personalización mediante hiperparámetros.\n",
        "* Limitaciones:\n",
        " * Propenso al sobreajuste: Especialmente en conjuntos de datos pequeños.\n",
        " * Requiere preprocesamiento: Aunque soporta características categóricas, puede ser necesario un procesamiento adicional en algunos casos.\n",
        "\n",
        "Usos Comunes:\n",
        "---\n",
        "* Regresión y Clasificación:\n",
        "Problemas tradicionales de aprendizaje supervisado.\n",
        "* Ranking:\n",
        "Muy utilizado en motores de búsqueda y sistemas de recomendación.\n",
        "* Competencias de Machine Learning:\n",
        "Regularmente destaca en competiciones de Kaggle.\n",
        "\n",
        "Para predecir ventas futuras utilizando LightGBM (LGBM), primero transformaremos las columnas categóricas (Producto, Fecha) y organizaremos los datos para ajustarse a un modelo supervisado. Esto implica crear características adicionales como tendencias temporales y factores de producto."
      ],
      "metadata": {
        "id": "mFr3F6W0EhD-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJUaJaIACi4n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Crear un DataFrame de ejemplo\n",
        "data = {\n",
        "    'Producto': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],\n",
        "    'Fecha': pd.date_range(start='2023-01-01', periods=8, freq='W'),\n",
        "    'Venta': [100, 200, 150, 220, 180, 250, 210, 300]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Procesar los datos\n",
        "# 1. Convertir fechas a componentes temporales\n",
        "df['Año'] = df['Fecha'].dt.year\n",
        "df['Mes'] = df['Fecha'].dt.month\n",
        "df['Semana'] = df['Fecha'].dt.isocalendar().week\n",
        "\n",
        "# 2. Codificar los productos\n",
        "df['Producto_Cod'] = df['Producto'].astype('category').cat.codes\n",
        "\n",
        "# 3. Crear el conjunto de características (X) y el objetivo (y)\n",
        "X = df[['Producto_Cod', 'Año', 'Mes', 'Semana']]\n",
        "y = df['Venta']\n",
        "\n",
        "# 4. Dividir los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear y entrenar el modelo LightGBM\n",
        "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
        "lgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predecir ventas futuras\n",
        "y_pred = lgb_model.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"MSE (Error Cuadrático Medio): {mse:.2f}\")\n",
        "\n",
        "# Comparar predicciones vs valores reales\n",
        "resultados = pd.DataFrame({'Real': y_test, 'Predicción': y_pred})\n",
        "print(resultados)\n",
        "\n",
        "# Graficar resultados\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(len(y_test)), y_test, marker='o', label='Real')\n",
        "plt.plot(range(len(y_pred)), y_pred, marker='x', label='Predicción')\n",
        "plt.title('Predicción de Ventas vs Ventas Reales', fontsize=14)\n",
        "plt.xlabel('Índice de muestra', fontsize=12)\n",
        "plt.ylabel('Ventas', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación del Código:\n",
        "---\n",
        "* Transformación de Datos:\n",
        "Se descompone la fecha en componentes útiles (Año, Mes, Semana) para capturar la estacionalidad y las tendencias temporales.\n",
        "Se codifica la columna categórica Producto en valores numéricos usando astype('category').cat.codes.\n",
        "* Modelo LGBMRegressor:\n",
        "Se utiliza LGBMRegressor porque el objetivo es predecir valores continuos (ventas).\n",
        "El modelo es entrenado con los datos de entrenamiento (X_train, y_train).\n",
        "* Evaluación del Modelo:\n",
        "Se calcula el Error Cuadrático Medio (MSE), que mide la precisión del modelo.\n",
        "Los valores reales y las predicciones se comparan en un DataFrame y en un gráfico.\n",
        "* Salida Esperada:\n",
        " * MSE:\n",
        "Un valor bajo indica un buen ajuste del modelo.\n",
        " * Tabla de Comparación:\n",
        "\n",
        "   | Real | Predicción|\n",
        "   |---|---|\n",
        "  |200 |  198.75|\n",
        "  | 250 |  245.10|\n",
        "\n",
        "* Gráfico:\n",
        "Muestra cómo las predicciones del modelo se alinean con las ventas reales.\n",
        "\n",
        "Siguientes Pasos:\n",
        "---\n",
        "* Optimización del Modelo:\n",
        "Ajustar hiperparámetros como n_estimators, learning_rate, o max_depth para mejorar el rendimiento.\n",
        "* Predicción a Futuro:\n",
        "Crear datos futuros (por ejemplo, semanas o meses siguientes) y usarlos como entrada al modelo entrenado.\n",
        "* Características Avanzadas:\n",
        "Incluir efectos de promociones, datos externos como clima, o eventos especiales."
      ],
      "metadata": {
        "id": "MKDwB401F9ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ajustar los hiperparámetros de un modelo de LightGBM puede mejorar significativamente su rendimiento. Esto se realiza explorando combinaciones de parámetros clave como la profundidad de los árboles, el número de iteraciones y la tasa de aprendizaje.\n",
        "\n",
        "A continuación, mostramos cómo realizar este ajuste utilizando GridSearchCV, una técnica de búsqueda en cuadrícula de scikit-learn:"
      ],
      "metadata": {
        "id": "NkF6ZqKSHGcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Crear un DataFrame de ejemplo\n",
        "data = {\n",
        "    'Producto': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],\n",
        "    'Fecha': pd.date_range(start='2023-01-01', periods=8, freq='W'),\n",
        "    'Venta': [100, 200, 150, 220, 180, 250, 210, 300]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Procesar los datos\n",
        "df['Año'] = df['Fecha'].dt.year\n",
        "df['Mes'] = df['Fecha'].dt.month\n",
        "df['Semana'] = df['Fecha'].dt.isocalendar().week\n",
        "df['Producto_Cod'] = df['Producto'].astype('category').cat.codes\n",
        "\n",
        "X = df[['Producto_Cod', 'Año', 'Mes', 'Semana']]\n",
        "y = df['Venta']\n",
        "\n",
        "# Dividir los datos\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definir el modelo base\n",
        "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
        "\n",
        "# Definir los hiperparámetros a buscar\n",
        "param_grid = {\n",
        "    'num_leaves': [15, 31, 50],          # Máximo número de hojas en un árbol\n",
        "    'learning_rate': [0.01, 0.05, 0.1], # Tasa de aprendizaje\n",
        "    'n_estimators': [50, 100, 150],     # Número de iteraciones (árboles)\n",
        "    'max_depth': [3, 5, -1]             # Profundidad máxima (-1 significa sin límite)\n",
        "}\n",
        "\n",
        "# Configurar la búsqueda en cuadrícula\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=lgb_model,\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',  # Métrica para minimizar\n",
        "    cv=3,                              # Validación cruzada con 3 particiones\n",
        "    verbose=1,\n",
        "    n_jobs=-1                          # Paralelizar el proceso\n",
        ")\n",
        "\n",
        "# Ajustar el modelo\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Mostrar los mejores parámetros y el error\n",
        "best_params = grid_search.best_params_\n",
        "best_score = -grid_search.best_score_  # Invertir el signo para obtener el MSE positivo\n",
        "print(\"Mejores hiperparámetros:\", best_params)\n",
        "print(f\"Mejor MSE: {best_score:.2f}\")\n",
        "\n",
        "# Entrenar el modelo con los mejores parámetros\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo final\n",
        "mse_final = mean_squared_error(y_test, y_pred)\n",
        "print(f\"MSE en el conjunto de prueba: {mse_final:.2f}\")\n",
        "\n",
        "# Graficar las predicciones\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(len(y_test)), y_test, marker='o', label='Real')\n",
        "plt.plot(range(len(y_pred)), y_pred, marker='x', label='Predicción')\n",
        "plt.title('Predicción de Ventas vs Ventas Reales', fontsize=14)\n",
        "plt.xlabel('Índice de muestra', fontsize=12)\n",
        "plt.ylabel('Ventas', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v8F2dBg8CpaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación del Ajuste de Hiperparámetros:\n",
        "\n",
        "* Hiperparámetros Ajustados:\n",
        " * num_leaves: Número máximo de hojas por árbol. Un valor mayor puede mejorar la capacidad del modelo pero aumenta el riesgo de sobreajuste.\n",
        " * learning_rate: Tasa de aprendizaje. Valores más bajos suelen requerir más iteraciones pero son más precisos.\n",
        " * n_estimators: Número de árboles en el modelo.\n",
        " * max_depth: Controla la profundidad máxima del árbol. Un valor de -1 significa sin restricciones.\n",
        "* GridSearchCV:\n",
        "Explora todas las combinaciones de los hiperparámetros definidos en param_grid utilizando validación cruzada.\n",
        "Selecciona la combinación con el menor error cuadrático medio negativo (neg_mean_squared_error).\n",
        "* Modelo Final:\n",
        "Después de encontrar los mejores parámetros, se reentrena el modelo en el conjunto de entrenamiento.\n"
      ],
      "metadata": {
        "id": "kMELtn_GHMm2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jBTeCd7UHhd-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}